Neural Nets Notes:

Inner product: dot product
Outer product: cross product?
    (a_1, a_2, a_3) * (b_1, b_2, b_3) = ((a1b1, a1b2, a1b3), (a2b1,a2b2...), ())

Mat Mult: <4X10> * <10X2> => <4X2>

Linear Independence: set of non-zero vectors is linearly independent:  Sigma(a_i*v_i) = 0 iff a_i = 0 for all i
    Pretty much means none of them are in a line
Chain rule is important

Gradient vector is vector of all partial dericatives

Directional Derivative: Gradient vector (dot) D where D is unit directional vector

First order necessary conditions(TFAE):
    Gradient vector is 0 for point to be local min or max
    For all directional vectors, directional derivative is zero

Convexity:
    ????? See slides

Second order sufficiency conditions
    1. All second derivatives are positive
    2. All points in tangent plane have function values <= objective function value
    3. Function is convex

Review Metrics and Norms

Mean Value Theorem:


error = output - desired
delta_value = -error * (1-output)


Symbolic Logic, Rules of Inference, Truth Value of Compound Statements, Perceptrons and Logic


Rule of Inference Truth Table for PS3

| A | B | C | A->B | B->C | (A->B) & (B->C) | A->C | [(A->B) & (B->C)]->(A->C)|
------------------------------------------------------------------------------
| 0 | 0 | 0 |   1  |   1  |        1        |   1  |           1              |
| 0 | 0 | 1 |   1  |   1  |        1        |   1  |           1              |
| 0 | 1 | 0 |   1  |   0  |        0        |   1  |           1              |
| 0 | 1 | 1 |   1  |   1  |        1        |   1  |           1              |
| 1 | 0 | 0 |   0  |   1  |        0        |   0  |           1              |
| 1 | 0 | 1 |   0  |   1  |        0        |   1  |           1              |
| 1 | 1 | 0 |   1  |   0  |        0        |   0  |           1              |
| 1 | 1 | 1 |   1  |   1  |        1        |   1  |           1              |

Perceptron Logic:

Dividing Line: Sigma(w*x) + theta > 0

Second order perceptron:
	high order functions(xy, x^2, etc)
	more powerful, not as easy to work with

Newton's Method
	Finding roots, iterative

Method of Steepest Descent:
	x_{k+1} = x_k - {eta}f'(x_k)

Simulated Annealing:
	heuristic, along with genetic algorithms, tabu search, adaptive memory programming
		need to balance accuracy, search time
		many possibilities
	convergence in probability (stationary property)
	convergence in distrubution (converges to global minimum)
	based on modeling a thermo system
	Metropolis formulated 'Equations of State'
		Provides
	Kirkpatrick and Cerny found way to apply equations


